\documentclass[11pt]{article}


\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{epsfig, amsmath, amsfonts, soul, color}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{fullpage}
\usepackage{placeins}

\title{Predictive tabular databases for automatic machine learning}
\author{Patrick Shafto, Vikash K. Mansinghka...}

\begin{document}
\maketitle

\section{Predictive database}

Consists of a language for interfacing with domain-general multidimensional density approximator e.g, CrossCat!

\section{Queries}

\subsection{Core inference}

\begin{itemize}

\item \texttt{initialize($M_c, M_r, T, i$)} $\rightarrow \{ M_c, M_r, X_L, X_D \}$. $M_c$ is the metadata for columns including bidirectional mappings between labels and indices, the modeltype---`asymmetric\_beta\_bernoulli' for binary data, `symmetric\_dirichlet\_discrete' for closed categorical data, `pitmanyor\_atom' for open categorical data, `poisson\_gamma' for frequency data, and `normal\_inverse\_gamma' for numeric data. $M_r$ is the metadata for rows including bidirectional mappings between labels and indices. $T$ is a table indicating the values for a collection of variables, and $i$ is an initialization (together, apart, or from\_the\_prior). It returns $X_L$ and $X_D$. $X_L$ is the latent state, including all information necessary for prediction. $X_D$ is a mapping between the latent state and the table $T$. 

\item \texttt{analyze($M_c, T, X_L, X_D, kernel\_list, n\_steps, c, r, max\_iterations, max\_time$)} $\rightarrow \{X'_L, X'_D\}$. 
$kernel\_list$ contains a list of the kernels to be run. This list may contain one or more of the following: `column\_partition\_hyperparameter', `column\_partition\_assignments', `component\_hyperparameters', `row\_partition\_hyperparameters', and/or `row\_partition\_assignments'.
$n\_steps$ is the number of times the $kernel\_list$ will be run. $c$ is an array that specifies which columns on which to run the kernels, by indices or `all'. This applies only to column kernels. $r$ is an array that specifies which rows on which to run the kernels and applies only to row kernels.
$max\_iterations$ is the maximum number of desired iterations and $max\_time$ is the maximum wall clock time before returning a state. \texttt{Analyze} returns a new latent state, $X'_L$, and a corresponding mapping between the state and the table, $X'_D$, based on the $min(max\_iterations, max\_time)$. 

\end{itemize}

\subsection{Data manipulation}

\hl{FIXME}

\begin{itemize}
\item \texttt{add\_row} \hl{FIXME}
\item \texttt{remove\_row} \hl{FIXME}
\item \texttt{add\_column} \hl{FIXME}
\item \texttt{remove\_column} \hl{FIXME}
\item \texttt{update\_cell} \hl{FIXME}
\item \texttt{validate} \hl{FIXME}
\item \texttt{row\_major} \hl{FIXME}
\item \texttt{column\_major} \hl{FIXME}

\end{itemize}

\subsection{Prediction}

\hl{Consider where to define things over a single versus multiple samples.}

\begin{itemize}

% SimplePredictiveSample
\item \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,q$)} $\rightarrow x$. $Y$ is a list of conditions (indices paired with values). 
$q$ is a list of queried variables.
\footnote{Note that $q$ is overloaded, representing both the indices of the queried variable and the variable.}
The output is a list sampled values $x$,
$$
x \sim Pr[q|X_L,Y].
$$

% SimplePredictiveProbability
\item \texttt{simple\_predictive\_probability($M_c,X_L,X_D,Y,Q$)} $\rightarrow p$. $Y$ is a list of conditioning conditions (indices paired with values), while  
$Q$ is a list of queried conditions (indices paired with values). 
The output, $p$, is the marginal predictive probability of the queried $Q$ given the conditions $Y$, assuming the model in $X_L$,
$$
p = Pr[Q | X_L,Y].
$$
The probabilities are approximated, for any given query, by summing over the categories. If the queried variable is continuous, the probability is approximated using the CDF (epsilon is based on floating point accuracy).

% Impute
\item \texttt{impute($M_c, X_L,X_D,Y,q,n$) } $\rightarrow e$. The output is a list of the expected values of the posterior predictive distribution for variables corresponding to the indices $q$, approximated using $n$ samples,
$$
e = E_{\textrm{Pr}[q | X_L,Y]}[q].
$$

\end{itemize}

\subsection{Statistics}

\begin{itemize}

% ConditionalEntropy
\item \texttt{conditional\_entropy($M_c, X_L,X_D,d_{given},d_{target}, [n, max\_time]$)} $\rightarrow e$. $d_{given}$ and $d_{target}$ are lists of column indices, $n$ is the number of predictive samples to base the estimate on, subject to $max\_time$ which is the maximum amount of time allowed for computation. The output is an estimate of the conditional entropy of $d_{target}$ given $d_{given}$,
\footnote{\texttt{conditional\_entropy} is analogous to regression.}
$$
e = H(d_{target}|d_{given},X_L).
$$
Entropy is a special case of \texttt{conditional\_entropy}, where $d_{given}$ is an empty list.

% PredictivelyRelated
\item \texttt{predictively\_related($M_c, X_L, X_D, d, %threshold, confidence, \newline
[n, max\_time]$)} $\rightarrow m$. 
$d$ is a column index. The output, $m$ is a list containing mutual information between all columns $d_*$ and the target, $d$,
$$
m = I(d,d_*|X_L).
$$
\texttt{predictively\_related} is defined in terms of \texttt{conditional\_entropy}. 
%Default values for the threshold and confidence are 0 and .5.
\footnote{\texttt{predictively\_related} can be thought of as a generalization of a correlation coefficient. \texttt{predictively\_related} identifies any dependency that is, on average, predictively useful. Two major differences between correlation and \texttt{predictively\_related} are that \texttt{predictively\_related} identifies both linear and non-linear relationships and is robust to outliers.}

% ContextualStructuralSimilarity
\item \texttt{contextual\_structural\_similarity($X_D, r, d)$} $\rightarrow s$. 
$r$ is a row index. $d$ is a list of column indices. The output $s$, is a list containing the similarity of each row to to the target, $r$,
\footnote{This captures an intuition similar to that behind of non-metric measures of distance. That is, if A is similar to B and B is similar to C, A need not be similar to C because the contexts, $d$, behind the similarities may differ.}

$$
s = Pr[ r=r_*| X^d_D],
$$
where $r=r_*$ indicates, for each other row $r_*$, whether it is the same as the target row, according to the underlying model, given the specified dimensions $d$.

% StructuralSimilarity
\item \texttt{structural\_similarity($X_D, r$)} $\rightarrow s$. The output $s$ is as for \texttt{contextual\_structural\_similarity}. 
Note: \texttt{structural\_similarity} is a special case of \texttt{contextual\_structural\_similarity}, where all columns are considered.
\footnote{This is analogous to measures of similarity in metric-based models, e.g.\ PCA, MDS, etc.}

% StructuralAnomalousnessColumns
\item \texttt{structural\_anomalousness\_columns($X_D$)} $\rightarrow a$. The output is list containing, for each column, an estimate of anomalousness, $a$, with respect to the inferred structural model of the data,
$$
a = Pr[ f \ne f_*  | X_D].
$$
This estimates the probability that the column is the only one of its kind.
\footnote{To my knowledge, neither the structural nor the predictive anomalousness queries have a direct analog in traditional statistics.}

% StructuralAnomalousnessRows
\item \texttt{structural\_anomalousness\_rows($X_D$)} $\rightarrow a$. The output is a list containing, for each row, an estimate of  anomalousness, $a$,
$$
a = Pr[ r \ne r_* | X_D].
$$
This estimates the probability that a row is the only one of its kind.

% PredictiveAnomalousness
\item \texttt{predictive\_anomalousness($M_c, X_L, X_D, T, q, n$)} $\rightarrow a$. The output is an estimate of the predictive anomalousness of the true values of the queried variables represented by the indices in, $q$, 
$$
a = Pr[T_q | X_L, X_D]^{-1}.
$$
This could be used to spot errors in data entry, as well as instances that simply deviate from (i.e.\ are least well predicted by) the model. Note: This can be implemented using \texttt{simple\_predictive\_probabilityQuery}.
We can define \texttt{predictive\_anomalousnessRow} and  \texttt{predictive\_anomalousnessColumn} as a sum of the anomalousness of each cell in each row or column.
\footnote{This has undesirable numerical properties. The range is from 1 to infinity. Change this to bounded range, ideally 0 to 1.}

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pseudocode for implementing queries with CrossCat}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The notation in this section is derived from Mansinghka, Shafto, et al, ``CrossCat: A fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data''. 

\subsection{Prediction}

\begin{itemize}
\item $M_c$ is the column metadata. This includes the model types for the data in each column.
\item $X_L$ is the model, including the prior parameters $\alpha$, the CRP priors, $\vec{\lambda}$, the hyperparameters for the likelihood models, and $\vec{\psi}$ and $\vec{\tau}$ the sufficient statistics for the CRPs and likelihoods. Let $\alpha_D$ represent the CRP prior on views and $\alpha_v$ represent the CRP prior on the categories in view $v$. 
Let $\vec{\lambda_d}$ represent the hyperparameters for likelihood model for dimension $d$. 
Let $\vec{\psi^v}$ represent the sufficient statistics for view $v$ (the count) and $\vec{\psi^v_c}$ represent the sufficient statistics for category $c$ in view $v$ (the count).
Let $\vec{\tau^d_c}$ represent the sufficient statistics summarizing the data from dimension $d$ in category $c$.

\item $X_D$ is the mapping between the model and the data, including $z$ and $y$. Let $z_d$ represent the assigned view for dimension $d$ and $y^v_r$ represent the category for row $r$ in view $v$.

\item $Y$ is a list of conditions, specified as indices $(r,d)$ paired with values $v$. Each pair in this list, specifies an entry for a previously unobserved value in the table, including potentially novel rows and dimensions, $T_{r,d}=v$. 

\item $q$ is a list of queried variables, specified as indices. 

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%
% Begin SimplePredictiveSample
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}
\hl{FIXME: [[SDL: I've changd notation to make it more verbose but descriptive]]}

\Input{$M_c, X_L, X_D, Y, queries$.}
\Output{A list of sampled values $x$.}
\BlankLine
\ForEach{$query \in queries$}{

Extract the queried row and dimension:\\
$row,column \leftarrow query$

Find all elements of $Y$ that apply to the same column:
\[Y' \leftarrow Y s.t. \; Y_{*,2}=column\]

Sample a view and category, allowing the possibility of choosing a new view and/or category: \[
Pr(v', y'| X_L, Y') \propto Pr(Y'|v', y',X_L)Pr(v',y'|X_L)
\] 
The assignment of views is deterministic in the case where $column$ is previously observed,
$v' \leftarrow z_d$. The assignment of categories is deterministic when $v'$ and $row$ are previously observed, 
$y' \leftarrow y^{v'}_r$.

Sample a value from the appropriate posterior predictive (PP) distribution: \[ 
x_i \sim  PP(\vec{\tau^{d}_c}) 
\] 
}

\caption{\texttt{simple\_predictive\_sample}}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%
% Begin SimplePredictiveProbability
%%%%%%%%%%%%%%%%%%%%%

% CONTINUOUS VERSION
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, Q$.}
\Output{A list of marginal probabilities $p$.}
\BlankLine

\ForEach{$i \in Q$}{

Extract the queried row, dimension, and value:\\
$r,d,x \leftarrow Q_i$

Find all elements of $Y$ that apply to the same dimension:
\[Y' \leftarrow Y s.t. \; Y_{*,2}=d\]

$p_i \leftarrow 0$

\For{categories in view (and a new category)}{
\If{isContinuous(d)}{
$pX_c = cdf(x+\epsilon|X_L,Y') - cdf(x-\epsilon|X_L,Y')$
}{
$pX_c = P(x|X_L,Y')$
}
$pCat_c = P(z_d=c|Y')$
%The assignment of views is deterministic in the case where $d$ is previously observed,
%$v' \leftarrow z_d$. The assignment of categories is deterministic when $v'$ and $r$ are previously observed, 
%$y' \leftarrow y^{v'}_r$.

}

$pCat = logsumexp(pCat)$
$p_i = \sum_c (pX \times pCat)$

}

% DISCRETE APPROXIMATION VERSION
%\begin{algorithm}[ht]
%\DontPrintSemicolon
%\LinesNumbered
%\Indp
%
%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}
%\SetKwInOut{Initialize}{initialize}
%
%\Input{$X_L$, $X_D$, $Y$, $Q$, $n$.}
%\Output{A list of marginal probabilities $p$.}
%\BlankLine
%
%\For{n iterations}{
%Sample from posterior predictive distribution: \\
%$x_i \leftarrow$ \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y',Q$)}.
%}
%
% NOTE: law of the iterated relevant for choosing the number of bins? sqrt(n*log(log(n)))
%            : typically sqrt(n) is used (per wiki page on histograms)
%Bin values of $x$, identifying the sets that are within $\epsilon$ of the queried values $u$: \\
%$A = \{ x | u - \epsilon < x < u + \epsilon \}$
%
%Approximate probability via histogram: \\
%$p \approx \frac{\sum_{x_i} \mathcal{I_A}}{n}$

\caption{\texttt{simple\_predictive\_probability}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin Impute
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, q, n$.}
\Output{A list of estimated expected values $e$.}
\BlankLine

\For{n iterations}{

Draw sample for each $i \in q$: \\
$x_j =$ \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,q$)}

}

Compute the expected values: \\
$e \leftarrow \frac{\sum_j x}{n}$

\caption{\texttt{impute}}
\end{algorithm}

\clearpage
\subsection{Statistics}

%%%%%%%%%%%%%%%%%%%%%
% Begin ConditionalEntropy
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L,X_D,d_{given},d_{target},n,max\_time$}
\Output{An estimate of the conditional entropy $e$.}
\BlankLine

Query a previously unobserved row for the given and target dimensions: \\
$q_{given} \leftarrow \{R+1, d_{given}\}$ \\
$q_{target} \leftarrow \{R+1, d_{target}\}$

\For{min($n, max\_time$)}{

Draw a sample from joint distribution: \\
$\{x_{given_i}, x_{target_i}\} \leftarrow$ \\
\texttt{simple\_predictive\_sample($M_c, X_L, X_D,\{ \}, \{q_{given},q_{target}\}$)}

$Q \leftarrow \{q_{given}, x_{given_i} \}$

Compute the marginal probability of the given dimensions: \\
$p_{given_i} \leftarrow $\texttt{simple\_predictive\_probability($M_c, X_L,X_D,\{\},Q,n$)}
% NOTE: n used twice to parameterize different functions

$Q \leftarrow \{ \{ q_{given}, x_{given_i}\}, \{q_{target}, x_{target_i} \} \}$ 

Compute the joint probability of the target and given dimensions: \\
$p_{joint_i} \leftarrow $\texttt{simple\_predictive\_probability($M_c, X_L,X_D,\{\},Q,n$)}

}

Compute expected value:
$$
e = \sum_{i} log \left( \frac{p_{given}}{p_{joint}} \right)
$$


% NOTE: we could define entropy as a primitive here and reuse it.
%            : $H(Y|X) = H(X,Y)-H(X)$

\caption{\texttt{conditional\_entropy}}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%
% Begin PredictivelyRelated
%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L,d,n$} %FIXME: generalize to use $threshold$,$confidence$}
\Output{A list of values indicating the mutual information $m$ between each dimension and the target.}
\BlankLine

\ForEach{$d' \in D$}{
% compute: $I(X;Y) = H(X) - H(X|Y)$

Compute conditional entropy: \\
$e_c =$ \texttt{conditional\_entropy($M_c, X_L,X_D,d,d',n$)}

Compute marginal entropy: \\
$e_m =$ \texttt{entropy($M_c, X_L,X_D,d',n$)}

$m_i = e_x - e_m$
}

%NOTE: for programming, it is important that the conditional and marginal are defined over the same samples, to avoid discrepancies when working with small numbers of samples.

% http://en.wikipedia.org/wiki/Mutual_information

\caption{\texttt{predictively\_related}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin ContextualStructuralSimilarity
%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$X_D,r,d$}
\Output{A list of values indicating the similarity $s$ between each row $r'$ and the target $r$.}
\BlankLine

$s \leftarrow 0$

% NOTE: this is the naive version where we simply tally up the number of  times they are the same. a more sophisticated version would compute CRP weights over the relevant features. 
\For{$r' \in R$}{
\For{$d' \in d$}{
\If{$y^{z_{d'}}_{r'} = y^{z_{d'}}_{r}$
}{
$s_{r'} = s_{r'}+1$
}
}
}

\caption{\texttt{contextual\_structural\_similarity}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin StructuralAnomalousnessColumns
%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$X_D$}
\Output{A list of values indicating the anomalousness $a$ for each column.}
\BlankLine

\ForEach{$d \in D$}{
\If{
$|z=z_d|=1$
}{
$a_d = a_d + 1$
}
}

\caption{\texttt{structural\_anomalousness\_columns}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin StructuralAnomalousnessRows
%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$X_D$}
\Output{A list of values indicating the anomalousness $a$ for each row.}
\BlankLine

\For{$r \in R$}{
\For{$d \in D$}{
\If{ $|y^{z_{d}}=y^{z_{d}}_{r}|=1$
}{
$a_{r} = a_{r}+1$
}
}
}
\caption{\texttt{structural\_anomalousness\_rows}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin PredictiveAnomalousness
%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c,X_L,X_D,T,q, n$}
\Output{A list of values indicating the anomalousness $a$ for each queried element.}
\BlankLine

%For each query, compute the SimplePredictiveProbability, and compute the multiplicative inverse.

\ForEach{$i \in q$}{
$x \leftarrow T_q$

$Q \leftarrow \{q, x\}$

$a_i \leftarrow$ \texttt{simple\_predictive\_probability($M_c, X_L,X_D,\{\},Q,n$)$^{-1}$}
}
\caption{\texttt{predictive\_anomalousness}}
\end{algorithm}

\clearpage 
\section{JSON specification for predictive databases}

\subsection{High-level specs for predictive DB interface}

The section provides a JSON schema for $M_c$ the column metadata, $M_r$ the row metadata, 
$X_L$ the latent state for the Predictive Database, and $X_D$ the mapping between the model and the data. 

$M_c$ is a list, where each element is a dict containing four keys: \texttt{name\_to\_idx}, \texttt{idx\_to\_name}, and \texttt{column\_metadata}.  
%
\texttt{name\_to\_idx} and \texttt{idx\_to\_name} convert from column names to indices and indices to 
column names, respectively. 
%
\texttt{column\_metadata} is an array, where each entry is a dict with three keys: \texttt{modeltype}, \texttt{value\_to\_code}, and \texttt{code\_to\_value}. \texttt{modeltype} maps to a string containing one of five possible model types: `asymmetric\_beta\_bernoulli', `normal\_inverse\_gamma', `pitmanyor\_atom', `symmetric\_dirichlet\_discrete', `poisson\_gamma'.
\texttt{value\_to\_code} maps between values and how they are encoded in the data. For instance, for the `binary' data type, this might map `True' to 1 and `False' to 0. For the `categorical' data types, this would function similarly, but with more possible values. For the `normal\_inverse\_gamma' and `poisson\_gamma' model types, \texttt{valueConverter} is empty because there is no conversion to be done. \texttt{code\_to\_value} maps back from codes to values. 

%Finally, the \texttt{likelihoodModel} key contains one of the following strings: `betaBernoilli', `multinomialDirichlet', or `normalInverseChisquared'. The choice of likelihood model is determined by the data type.
 
$M_r$ is an array, where entries are dicts with two keys: \texttt{name\_to\_idx} and \texttt{idx\_to\_name}. These map from row names to row indices and back. 

For instance, consider the following data table: 
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c c c}
		& Height(in)		& Gender		& Nationality	& IQ  \\
Bob		&	66			& Male		& US			& 105 \\
Steve	& ?				& Male		& Canadian	& 100 \\ 
Jill		& 60				& Female		& US			& 104 \\
\end{tabular}
\end{center}
\end{table}
\FloatBarrier

$M_c$ and $M_r$ could be:
\begin{verbatim}
M_c: {
  �name_to_idx�: {0: �Height�, 1: �Gender�, 2: �Nationality�, 3: �IQ�}
  �idx_to_name�: {�Height�:0, �Gender�: 1, �Nationality�: 2, �IQ�: 3}
  �column_metadata�: [
    {�modeltype�: �normal_inverse_gamma�, �value_to_code�: {}, �code_to_value�: {}},
    {�modeltype�: �asymmetric_beta_bernoulli�, �value_to_code�: {0:�Male�, 1:�Female�}, ...
    	�code_to_value�: {�Male�:0, �Female�:1}},
    {�modeltype�: �symmetric_dirichlet_discrete�, �code_to_value�: {�US�:0, �Canadian�:1}, ...
    	�value_to_code�: {1:�Canadian�, 0:�US�}},
    {�modeltype�: �normal_inverse_gamma�, �value_to_code�: {}, �code_to_value�:{}}
    ]
}

M_r: {
  �name_to_idx�: {�Bob�:0, �Steve�:1, �Jill�:2},
  �idx_to_name�: {0:�Bob�, 1:�Steve�, 3:�Jill�},
}
\end{verbatim}

The latent state, $X_L$, is a dict with three keys:  
\texttt{column\_partition},
\texttt{column\_hypers}, and
\texttt{view\_state}.
%
\texttt{column\_partition} is a dict with three keys: \texttt{hypers}, \texttt{assignments}, and \texttt{counts}.
\texttt{hypers} is a dict containing a single key, \texttt{alpha}, which maps to a number representing the concentration parameter of the CRP. \texttt{assignments} is an array, where each element is an integer representing the view to which that column is assigned. \texttt{counts} is an array with length equal to the number of views, where each entry is the number of columns in that view. 

\texttt{column\_hypers} is an array where each element is a dict whose structure depends on the \texttt{modeltype} of that column. For instance, for a column with a `asymmetric\_beta\_bernoulli' modeltype, the dict would have three keys: \texttt{log\_strength} and\texttt{balance}. One additional key would apply to all model types. \texttt{fixed} would map to the strings `true' or `false' to indicate whether the parameters were fixed or not.

\texttt{view\_state} is an array where each element is a a dict with three keys: \texttt{column\_names}, \texttt{row\_partition\_model} and \texttt{column\_component\_suffstats}. 
\texttt{column\_names} is an array of strings where the indices are locally consistent (within the view). \hl{FIXME: [[SDL: I propose indices instead of names]]}
\texttt{row\_partition\_model} is a dict with two keys: \texttt{hypers} and \texttt{counts}. \texttt{hypers} is a dict with a single key, \texttt{alpha}, which maps to a number representing the concentration parameter for the row CRP in that view. \texttt{counts} is an array, with length equal to the number of categories, where each element is the number of rows assigned to that category.
%
\texttt{column\_component\_suffstats} is an array of arrays of dicts, where the structure of each dict depends on the model type. The outer array is over the columns while the inner array is over categories. Each dict contains the sufficient statistics that are appropriate for the model type of the column.  For instance, for an `asymmetric\_beta\_bernoulli' column, the dict would have three keys: \texttt{0\_count}, \texttt{1\_count}, and \texttt{N}.

Assume the underlying crosscat state is:
\begin{verbatim}
views: [ 1 1 2 3]
categories: [
	[1 1 2]
	[1 2 1]
	[1 1 1]
]
\end{verbatim}

Then, $X_L$ could be: 
\begin{verbatim}
X_L: {
  �column_partition�: {
	�hypers�: {�log_alpha�: 1.1},
	�assignments�: [0, 0, 1, 2],
	�counts�: [2, 1, 1],
},
  �column_hypers�: [
	{�fixed�: false, �mu�: 63.5, �log_kappa�: 8.2, �log_alpha�: 2.3, �log_beta�: 3.4},
	{�fixed�: false, �log_strength�: 10, �balance�: 1.2}
	{�fixed�: true, �log_alpha�: .5, �K�: 2},
	{�fixed�: false, �mu�: 101.1, �log_kappa�: 4, �log_alpha�: 10.1, �log_beta�: 5}
	],
  �view_state�: [
	{�row_partition_model�: {�hypers�: {�log_alpha�:3.2}, �counts�:[2, 1]}, ...
		"column_names": ["Height(in)", "Gender"], ...
		�column_component_suffstats�: [[{�sum_x�: 66.0, �sum_x_sq�: 4356.0},...
		{�sum_x�: 60.0, �sum_x_sq�: 3600.0}], [{�0_count�: 2, �1_count�: 0}, ...
			{�0_count�: 0, �1_count�: 1}]]},
	{�row_partition_model�: {�hypers�: {�log_alpha�:2.2}, �counts�:[2, 1]}, ...
		"column_names": ["Nationality"], ...
		�column_component_suffstats�: [[{�0_count�: 2, �1_count�:0},{�0_count�: 2, �1_count�:0}]]},
	{�row_partition_model�: {�hypers�: {�log_alpha�:0.8}, �counts�:[3]}, ...
		"column_names": ["IQ"], ...
		�column_component_suffstats�: [[{�sum_x�: 309.0, �sum_x_sq�: 31481.0}]]}
	]
}
\end{verbatim}

The mapping between the latent state and the data, $X_D$, is an array of arrays. The elements of the outer array correspond to views and the inner array corresponds to the rows. Entries are integers indicating, for a row in a view, which category it belongs to.

\begin{verbatim}
X_D: [
	[0, 0, 1],
	[0, 1, 0],
	[0, 0, 0]
]
\end{verbatim}

\hl{FIXME: separate low-level definition??}

\end{document} 
