\documentclass[11pt]{article}


\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{epsfig, amsmath, amsfonts, soul, color}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{fullpage}
\usepackage{placeins}

\title{Predictive tabular databases for automatic machine learning}
\author{Patrick Shafto, Vikash K. Mansinghka...}

\begin{document}
\maketitle

\section{Predictive database}

Consists of a language for interfacing with domain-general multidimensional density approximator e.g, CrossCat!

\subsection{Terminology}

\begin{itemize}
\item $T$ is the data table.
\item $M_c$ is the column metadata. This includes the model types for the data in each column.
\item $X_L$ is the model, comprised of hyperparameters and sufficient statistics for CRP models and component models. CRP model hyperparameters are denoted by $\alpha$ and sufficient statistics are denoted by $\vec{\psi}$.  Component model hyperparameters are denoted by $\vec{\lambda}$ and sufficient statistics are denoted by and $\vec{\tau}$.  Subscripts and superscripts specify applicability to views, columns or clusters.
\begin{itemize}

\item $\alpha_D$: CRP prior on views.

\item $\vec{\psi}^v$: sufficient statistics (the count of columns) for view $v$.

\item $\alpha_v$: CRP prior on categories in view $v$.

\item $\vec{\psi}_c^{v}$: sufficient statistics (the count of rows) for category $c$ in view $v$.

\item $\vec{\tau}_{c}^{d}$: sufficient statistics summarizing the data from category $c$ in dimension $d$.  Note: the shape of the sufficient statistics varies by component model type (categorical, continuous, etc).
\end{itemize}

\item $X_D$ is the mapping between the model and the data. Let $z_d$ represent the assigned view for dimension $d$ and $c^v_r$ represent the category for row $r$ in view $v$.

\item $Y$ is a list of individual conditions, $y$.  Each condition is a tuple of row, dimension and value, $y=(r, d, v)$, specifying an entry for a previously unobserved value in the table, including potentially novel rows and dimensions, $T_{r,d}=v$.  The $5^{th}$ condition's row would by specified by $Y_{5,0}$.  The columns for all conditions would be specified by $Y_{*,1}$.

\item $Q$ is a list of queried values, $q$.  Each queried value is specified as a tuple of row and dimensions, $q=(r, d)$. 

\end{itemize}

\section{Queries}

\subsection{Core inference}

\begin{itemize}

\item \texttt{initialize($M_c, M_r, T, i$)} $\rightarrow \{ X_L, X_D \}$. \\
$M_c$ is the metadata for columns including bidirectional mappings between labels and indices, the modeltype---`asymmetric\_beta\_bernoulli' for binary data, `symmetric\_dirichlet\_discrete' for closed categorical data, `pitmanyor\_atom' for open categorical data, `poisson\_gamma' for frequency data, and `normal\_inverse\_gamma' for numeric data. $M_r$ is the metadata for rows including bidirectional mappings between labels and indices. $T$ is a table indicating the values for a collection of variables, and $i$ is an initialization (together, apart, or from\_the\_prior). It returns $X_L$ and $X_D$. $X_L$ is the latent state, including all information necessary for prediction. $X_D$ is a mapping between the latent state and the table $T$. 

\item \texttt{analyze($M_c, T, X_L, X_D, kernel\_list, n\_steps, c, r$)} $\rightarrow \{X'_L, X'_D\}$. \\
$kernel\_list$ contains a list of the kernels to be run. This list may contain one or more of the following: `column\_partition\_hyperparameter', `column\_partition\_assignments', `column\_hyperparameters', `row\_partition\_hyperparameters', and/or `row\_partition\_assignments'.
$n\_steps$ is the number of times the $kernel\_list$ will be run. $c$ is an array that specifies which columns on which to run the kernels, by indices or `all'. This applies only to column kernels. $r$ is an array that specifies which rows on which to run the kernels and applies only to row kernels.
\texttt{analyze} returns a new latent state, $X'_L$, and a corresponding mapping between the state and the table, $X'_D$.

\end{itemize}

\subsection{Data manipulation}

\subsection{Prediction}

\begin{itemize}

% SimplePredictiveSample
\item \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,r, D$)} $\rightarrow x$. \\
$D$ is a list of columns on a single row $r$ to be simultaneously sampled given the conditions $Y$.  The output is a list sampled values $x$ with the same ordering as $D$,
$$
x \sim Pr[T_{r, D}|X_L,Y].
$$
\hl{NOTE:} even if a queried value, $T_{r,d}$, is observed, it will be sampled from the model specified by $X_L, Y$

% Impute
\item \texttt{impute($M_c, X_L,X_D,Y,q,n$) } $\rightarrow e$. \\
The output is a list of the expected values of the posterior predictive distribution for variables corresponding to the indices $q$, approximated using $n$ samples,
$$
e = E_{\textrm{Pr}[T_q | X_L,Y]}[T_q].
$$

\end{itemize}

\subsection{Statistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pseudocode for implementing queries with CrossCat}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The notation in this section is derived from Mansinghka, Shafto, et al, ``CrossCat: A fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data''. 

\subsection{Prediction}


%%%%%%%%%%%%%%%%%%%%%
% Begin SimplePredictiveSample
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, r, D$.}
\Output{A list of sampled values $x$.}
\BlankLine
\tcp{Separate conditions, $Y$, by whether they apply to the queried row, $r$}
$Y^q \leftarrow Y s.t. \; Y_{*,1}=r$ \;
$Y^o \leftarrow Y s.t. \; Y_{*,1}!=r$ \;
$views_{sampling} \leftarrow \{get\_view(X_L, d)\}_{d \in D}$ \;
\ForEach{$view_i \in views_{sampling} $}{
  \eIf{$r$ denotes an observed row}{
    \tcp{The category is determinstically set}
    $c_{view_i} \leftarrow get\_category(X_D, view_i, r)$ \;
  }{
    \tcp{sample a category, allowing the possibility of a new category}
    $Pr(c | view, Y^q, Y^o, X_L) \propto Pr(Y^q | c, view, Y^o, X_L)Pr(c|view, X_L)$ \;
    $c_{view_i} \sim Pr(c | view_i, Y^q, Y^o, X_L)$ \;
  }
}
\ForEach{$d \in D$}{
  $view_i \leftarrow get\_view(X_L, d)$ \;
  $cm \leftarrow get\_component\_model(X_L, view_i, c_{view_i}, d)$ \;
  \tcp{Determine conditions that modify this component model}
  $Y^c = Y s.t. \; Y_{*,1} = d \; and \; get\_category(X_D, view_i, Y_{*,2}) = c_{view_i}$ \;
  \tcp{Update the component model}
  $cm' \leftarrow update\_component\_model(cm, Y^c)$ \;
  \tcp{Sample a value from the posterior predictive (PP) distribution}
  $x_i \sim  PP(cm')$ \;
}  


\caption{\texttt{simple\_predictive\_sample}}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%
% Begin Impute
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, q, n$.}
\Output{A list of estimated expected values $e$.}
\BlankLine

\For{n iterations}{

Draw sample for each $i \in q$: \\
$x_j =$ \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,q$)}

}

Compute the expected values: \\
$e \leftarrow \frac{\sum_j x}{n}$

\caption{\texttt{impute}}
\end{algorithm}

\clearpage
\subsection{Statistics}

\clearpage 
\section{JSON specification for predictive databases}

\subsection{High-level specs for predictive DB interface}

The section provides a JSON schema for $M_c$ the column metadata, $M_r$ the row metadata, 
$X_L$ the latent state for the Predictive Database, and $X_D$ the mapping between the model and the data. 

$M_c$ is a list, where each element is a dict containing four keys: \texttt{name\_to\_idx}, \texttt{idx\_to\_name}, and \texttt{column\_metadata}.  
%
\texttt{name\_to\_idx} and \texttt{idx\_to\_name} convert from column names to indices and indices to 
column names, respectively. 
%
\texttt{column\_metadata} is an array, where each entry is a dict with three keys: \texttt{modeltype}, \texttt{value\_to\_code}, and \texttt{code\_to\_value}. \texttt{modeltype} maps to a string containing one of five possible model types: `asymmetric\_beta\_bernoulli', `normal\_inverse\_gamma', `pitmanyor\_atom', `symmetric\_dirichlet\_discrete', `poisson\_gamma'.
\texttt{value\_to\_code} maps between values and how they are encoded in the data. For instance, for the `binary' data type, this might map `True' to 1 and `False' to 0. For the `categorical' data types, this would function similarly, but with more possible values. For both `categorical' and `binary' data types, values must start from zero and increment up by 1. \texttt{code\_to\_value} maps back from codes to values. For the `normal\_inverse\_gamma' and `poisson\_gamma' model types, \texttt{value\_to\_code} and \texttt{code\_to\_value} are empty because there is no conversion to be done.

%Finally, the \texttt{likelihoodModel} key contains one of the following strings: `betaBernoilli', `multinomialDirichlet', or `normalInverseChisquared'. The choice of likelihood model is determined by the data type.
 
$M_r$ is an array, where entries are dicts with two keys: \texttt{name\_to\_idx} and \texttt{idx\_to\_name}. These map from row names to row indices and back. 

For instance, consider the following data table: 
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c c c}
		& Height(in)		& Gender		& Nationality	& IQ  \\
Bob		&	66			& Male		& US			& 105 \\
Steve	& ?				& Male		& Canadian	& 100 \\ 
Jill		& 60				& Female		& US			& 104 \\
\end{tabular}
\end{center}
\end{table}
\FloatBarrier

$M_c$ and $M_r$ could be:
\begin{verbatim}
M_c: {
  “name_to_idx”: {0: “Height”, 1: “Gender”, 2: “Nationality”, 3: “IQ”}
  “idx_to_name”: {“Height”:0, “Gender”: 1, “Nationality”: 2, “IQ”: 3}
  “column_metadata”: [
    {“modeltype”: “normal_inverse_gamma”, “value_to_code”: {}, “code_to_value”: {}},
    {“modeltype”: “asymmetric_beta_bernoulli”, “value_to_code”: {0:”Male”, 1:”Female”}, ...
    	“code_to_value”: {“Male”:0, “Female”:1}},
    {“modeltype”: “symmetric_dirichlet_discrete”, “code_to_value”: {“US”:0, “Canadian”:1}, ...
    	“value_to_code”: {1:”Canadian”, 0:”US”}},
    {“modeltype”: “normal_inverse_gamma”, “value_to_code”: {}, “code_to_value”:{}}
    ]
}

M_r: {
  “name_to_idx”: {“Bob”:0, “Steve”:1, “Jill”:2},
  “idx_to_name”: {0:”Bob”, 1:”Steve”, 3:”Jill”},
}
\end{verbatim}

The latent state, $X_L$, is a dict with three keys:  
\texttt{column\_partition},
\texttt{column\_hypers}, and
\texttt{view\_state}.
%
\texttt{column\_partition} is a dict with three keys: \texttt{hypers}, \texttt{assignments}, and \texttt{counts}.
\texttt{hypers} is a dict containing a single key, \texttt{alpha}, which maps to a number representing the concentration parameter of the CRP. \texttt{assignments} is an array, where each element is an integer representing the view to which that column is assigned. \texttt{counts} is an array with length equal to the number of views, where each entry is the number of columns in that view. 

\texttt{column\_hypers} is an array where each element is a dict whose structure depends on the \texttt{modeltype} of that column. For instance, for a column with a `asymmetric\_beta\_bernoulli' modeltype, the dict would have two keys: \texttt{strength} and \texttt{balance}. One additional key would apply to all model types. \texttt{fixed} would map to the strings `true' or `false' to indicate whether the parameters were fixed or not.

\texttt{view\_state} is an array where each element is a a dict with three keys: \texttt{column\_names}, \texttt{row\_partition\_model} and \texttt{column\_component\_suffstats}. 
\texttt{column\_names} is an array of strings where the indices are locally consistent (within the view).

\texttt{row\_partition\_model} is a dict with two keys: \texttt{hypers} and \texttt{counts}. \texttt{hypers} is a dict with a single key, \texttt{alpha}, which maps to a number representing the concentration parameter for the row CRP in that view. \texttt{counts} is an array, with length equal to the number of categories, where each element is the number of rows assigned to that category.
%
\texttt{column\_component\_suffstats} is an array of arrays of dicts, where the structure of each dict depends on the model type. The outer array is over the columns while the inner array is over categories. Each dict contains the sufficient statistics that are appropriate for the model type of the column.  For instance, for an `asymmetric\_beta\_bernoulli' column, the dict would have three keys: \texttt{0\_count}, \texttt{1\_count}, and \texttt{N}.  Continuous model types' suffstats must include a value \texttt{N} for the count of the number of non-empty values.

Assume the underlying crosscat state is:
\begin{verbatim}
views: [ 1 1 2 3]
categories: [
	[1 1 2]
	[1 2 1]
	[1 1 1]
]
\end{verbatim}

Then, $X_L$ could be: 
\begin{verbatim}
X_L: {
  "column_partition": {
	"hypers": {"alpha": 1.1},
	"assignments": [0, 0, 1, 2],
	"counts": [2, 1, 1],
},
  "column_hypers": [
	{"fixed": false, "mu": 63.5, "kappa": 8.2, "alpha": 2.3, "beta": 3.4},
	{"fixed": false, "strength": 10, "balance": 1.2}
	{"fixed": true, "alpha": .5, "K": 2},
	{"fixed": false, "mu": 101.1, "kappa": 4, "alpha": 10.1, "beta": 5}
	],
  "view_state": [
	{"row_partition_model": {"hypers": {"alpha":3.2}, "counts":[2, 1]}, ...
		"column_names": ["Height(in)", "Gender"], ...
		"column_component_suffstats": [[{"sum_x": 66.0, "sum_x_sq": 4356.0, "N": 1},...
		{"sum_x": 60.0, "sum_x_sq": 3600.0, "N": 1}], [{"0_count": 2, "1_count": 0, "N": 2}, ...
			{"0_count": 0, "1_count": 1, "N": 1}]]},
	{"row_partition_model": {"hypers": {"alpha":2.2}, "counts":[2, 1]}, ...
		"column_names": ["Nationality"], ...
		"column_component_suffstats": [[{"0_count": 2, "1_count":0, "N": 2},...
		{"0_count": 2, "1_count":0, "N": 2}]]},
	{"row_partition_model": {"hypers": {"alpha":0.8}, "counts":[3]}, ...
		"column_names": ["IQ"], ...
		"column_component_suffstats": [[{"sum_x": 309.0, "sum_x_sq": 31481.0, "N": 3}]]}
	]
}
\end{verbatim}

The mapping between the latent state and the data, $X_D$, is an array of arrays. The elements of the outer array correspond to views and the inner array corresponds to the rows. Entries are integers indicating, for a row in a view, which category it belongs to.

\begin{verbatim}
X_D: [
	[0, 0, 1],
	[0, 1, 0],
	[0, 0, 0]
]
\end{verbatim}

\end{document} 
